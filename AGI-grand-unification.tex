\input{../YKY-preamble.tex}
\setmainfont[BoldFont=Alibaba_Sans_Regular.otf,ItalicFont=Alibaba_Sans_Light_Italic.otf]{Alibaba_Sans_Light.otf}
	
\usepackage[active,tightpage]{preview}		% for continuous page(s)
\renewcommand{\PreviewBorder}{0.5cm}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}

\usepackage[absolute,overlay]{textpos}		% for page number on upper left corner

\usepackage{color}
\usepackage{mathtools}
\usepackage[hyperfootnotes=false]{hyperref}

% \usepackage[backend=biber,style=numeric]{biblatex}
% \bibliography{../AGI-book}
% \renewcommand*{\bibfont}{\footnotesize}

\usetikzlibrary{shapes}
\usepackage[export]{adjustbox}				% ??
\usepackage{verbatim} % for comments
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \titleformat{\subsection}[hang]{\bfseries\large\color{blue}}{}{0pt}{} 
% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%

\newcommand\reduline{\bgroup\markoverwith{\textcolor{red}{\rule[-0.5ex]{2pt}{0.4pt}}}\ULon}

%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
% \setmathfont{Latin Modern Math}
\DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=red,colback=pink,
	highlight math style={enhanced}
}
\newcommand{\atom}{\vcenter{\hbox{\tcbox{....}}}}

\let\oldtextbf\textbf
\renewcommand{\textbf}[1]{\textcolor{blue}{\oldtextbf{#1}}}

\newcommand{\logic}[1]{{\color{violet}{\textit{#1}}}}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\token}{\vcenter{\hbox{\includegraphics[scale=1]{token.png}}}}
\newcommand{\proposition}{\vcenter{\hbox{\includegraphics[scale=0.8]{proposition.png}}}}

\begin{document}

\begin{preview}

\cc{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\LARGE AGI 大统一理论}}
}{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\LARGE AGI Grand Unification}}
}

% \author{YKY} % Your name
\date{\vspace{-2cm}} % Date, can be changed to a custom date

\maketitle

\setcounter{section}{-1}

% (1) Circled page number on upper left corner
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small 1}}}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\section{综述}

\begin{itemize}

	\item 大统一理论是在 \textbf{强化学习} 的框架下进行的，这是以 Richard Sutton 为代表人物 提出的理论框架。 

	\item 在 强化学习里 最辣手的一个问题，就是如何 储存和计算 所有 \textbf{状态} 之上的 \textbf{概率分布}。 对 AGI 来说，状态 = 思维空间。 我们需要的是 所有可能的思维之上的概率分布，而这是 AGI 的一个硬性需求，无法避免。 由于思维空间是高维的向量空间，它上面的概率分布是一个庞大的 mathematical object，很难在计算机上表示。 如果用 神经网络 表示，则问题是如何对这个概率分布进行 \textbf{采样} (sampling), 在神经网络里，这是很困难的。

	\item \textbf{Hopfield 网络}的权重 定义了一个 能量地势 (energy landscape)，它可以看成是一个 implicit 的 \textbf{概率分布}。 透过 Hopfield 网络的 learning，可以改变这个概率分布。 \reduline{但这需要修改 Hopfield 网络的算法，将 能量 诠释成 概率}，而这正是 \textbf{Boltzmann machine}，也称作 EBM (Energy-Based Models).

	\item 根据 ``Hopfield Network is All You Need'' 论文\footnote{感谢 Eric Zeng 给我推荐这篇论文。}，现代 Hopfield 网络的 state update rule 跟 \textbf{Transformer} 重合\footnote{注意这是 state update rule 而不是 learning update rule.  前者 更改 Hopfield 网络的 激活 状态； 后者 更改 Hopfield 网络的权重／记忆。}。 换句话说，每执行一次 Transformer，就会趋向 Hopfield 的能量最低点。 

	\item Transformer 的 softmax 可以看成是 \textbf{大脑}中某种 ``winner-takes-all'' 机制。 从这个角度，可以类比大脑思考的机制，互相参考以获取更多灵感。
	
	\item 我最新的论文 提出，Transformer 具有 \textbf{逻辑结构}，可以在逻辑基础上建立 AGI.

\end{itemize}


\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{AGI 大统一理论}{AGI Unification}}
\end{textblock*}
\vspace*{0.3cm} 

\section{Hopfield 网络}

Hopfield 网络是一种 特别简单的 fully-connected 神经网络，它具有 \textbf{associative memory} 的特性，可以凭部分 pattern 回忆整个 pattern:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Hopfield-network-example.png}}}
\end{equation}
记忆由神经元之间的 \textbf{连接权重} 决定，而这些权重定义了一个 \textbf{能量函数}。 Hopfield 网络就是统计物理学里面的 \textbf{Ising model} 应用到了神经网络上。 

可以将一个 Hopfield 网络 铺平 画成这样:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Hopfield-network-1.png}}}
\label{fig:Hopfield-network}
\end{equation}
注意它不是 feed-forward 网络，它的 输入 和 输出 在同一地方。

\subsection{经典 Hopfield 网络}

我们的符号跟随 \textit{Hopfield Network is All You Need}.

$\vect{x}^i$ = 需要记忆的 \textbf{patterns} (有 $N$ 个), $x^i_{s}$ 是它的 $s$-th bit.

$\vect{X} = (\vect{x}^1, ... , \vect{x}^N)$ 是所有 patterns 的矩阵。

$\vect{\xi}$ = 网络的 \textbf{状态}，$\xi_s$ = $s$-th 神经元 的 激活状态。

\textbf{连接权重} between $s$-th and $t$-th neurons:
\begin{equation}
\boxed{\mbox{Weights}} \quad T_{s,t} = \sum_i x^i_s x^i_t
\end{equation}

\textbf{总能量} (Hamiltonian):
\begin{equation}
\boxed{\mbox{Energy}} \quad E = -\frac{1}{2} \sum_s \sum_{t \neq s} \xi_s T_{s,t} \xi_t
\end{equation}

\subsection{Modern Hopfield 网络}

在经典 Hopfield 网络里，当 A 和 B 两个 patterns 太靠近的时候，它们会互相干扰，导致可以储存的 patterns 数量不大。  现代 Hopfield 网络 改变 Hamiltonian 能量函数，令干扰减弱，可以储存数量更多的 patterns：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{modern-Hopfield-network-energy-landscape.png}}}
\end{equation}

\textbf{新的能量}函数:
\begin{equation}
\boxed{\mbox{New Energy}} \quad E = - \sum_i F( \vect{\xi}^T \vect{x}^i )
\end{equation}
注： $ \displaystyle \vect{\xi}^T \vect{x}^i = \sum_s \xi_s x^i_s$ ，\; $F$ = interaction function.

[Demircigil et al 2017] 提出 $F$ 用 exponential 函数。

\textbf{State update rule}:
\begin{equation}
\vect{\xi}^{\mbox{new}} = \vect{X} \; \mbox{softmax}(\beta \vect{X}^T \vect{\xi})
\end{equation}

\subsection{Hopfield-Transformer 对应}

\textbf{传统 Transformer}'s state update rule:
\begin{equation}
\vect{Z} = \mbox{softmax} \left( \frac{1}{\sqrt{d_k}} \vect{Q} \, \vect{K}^T \right) \vect{V}
\end{equation}

\textbf{Modern Hopfield network}'s state update rule:
\begin{equation}
\vect{Z} = \mbox{softmax} \left( \beta \vect{\hat{R}} \vect{\hat{Y}}^T \right) \vect{\hat{Y}}^T
\end{equation}
这里 $\vect{\hat{R}}, \vect{\hat{Y}}, \vect{\hat{X}}$ 是 $\vect{R}, \vect{Y}, \vect{X}$ 分别乘上了适当的 $\vect{W}$'s 矩阵，但为了数式的简洁，使用了代号。

Query patterns = $\vect{R} = (\vect{r}_1, ... , \vect{r}_M)$ 是网络的\textbf{状态}。 之所以有 $M$ 个状态，是因为他们将 Transformer 的 $M$ 个输入 \textbf{摊开}来，才构成一个大的 Hopfield 网。 \reduline{不这样做根本无法将 Hopfield 网和 Transformer 等同起来}。

$\vect{Y}$ 就是 Hopfield 记忆里的 patterns，它们担任 Transformer 里 \textbf{keys} 的角色。

在 Self-Attention 里，$\vect{R} = \vect{Y}$.

跟图 (\ref{fig:Hopfield-network}) 比较，这个是 受 Transformer 结构 约束的 Hopfield 网络：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{Hopfield-network-as-Transformer.png}}}
\end{equation}

\subsection{Boltzmann 机}

注意： 以下是 Boltzmann machine 跟 \textbf{经典} Hopfield network 的对应。

Let $O = (O_1, ..., O_n)$ be the \textbf{state vector}.

$W = \{ W_{s,t} \}$ are connection \textbf{weights}.

\textbf{State update rule}:  $i$-th unit is set to 1 with probability
\begin{equation}
\frac{1}{1 + e^{- S_i / T}}
\end{equation}
where $T$ is a temperature.

如果用以上的 update rule，则 Hopfield 能量 变成 \textbf{概率分布}：
\begin{equation}
P(O) = P(O|W) = \frac{e^{-\mathcal{E}(O)/T}}{Z} \quad \boxed{\mbox{Boltzmann distribution}}
\end{equation}
where partition function $\displaystyle Z = \sum_U e^{-\mathcal{E}(U)/T} $

{\color{red}TO-DO:} \reduline{求出 现代版的 Hopfield network 的 Boltzmann state update rule}.


{\color{red}TO-DO:} \reduline{求出 Hopfield-Boltzmann machine 的 \textbf{learning} update rule}.  但它似乎是根据 \textbf{记忆} 而 update 的？？

\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{AGI 大统一理论}{AGI Unification}}
\end{textblock*}
\vspace*{0.3cm} 

\section{与强化学习 结合}

\subsection{LLM 内部很可能存在冗余}

LLM (大型语言模型) 是根据 自回归 (auto-regression, 强迫模型的输出跟输入一样) 的原理训练。 在这种模式下，Transformer 用大约一半的资源，将输入句子转化成一个 \reduline{具有高度抽象语义的 内在表示}。 它的后半部 利用这个抽象表示 去预测掩盖了的词语：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{redundancy-in-LLM-1.png}}}
\end{equation}
在一个智能系统里面，其实我们最需要的是 中间的那个 hidden representation，用来做「下游」的工作，但不一定是预测词语，所以后半部对智能系统来说，构成了一种\textbf{冗余}。

有两个做法： 一是接受这个冗余，继续将整个 AGI 系统做出来。 二是改变系统架构，重新训练一个更合乎直观的模型。

个人认为 强化学习 模型 更为直观，可以省却很多理解上的「拐弯抹角」，令 AGI 的设计更清晰：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{redundancy-in-LLM-2.png}}}
\end{equation}
这个 transition function 就是一个类似 Transformer 的函数。

\subsection[Transformer]{Transformer $\rightarrow$ Hopfield $\rightarrow$ Boltzmann}

在强化学习里，系统需要 \textbf{储存} 并 \textbf{学习} 一个分布在所有状态或动作（包括 思维动作，即「思维空间」）之上的 \textbf{概率分布}。 由于思维空间是一个庞大的 高维向量空间，在计算机上很难处理。 （数学家们很轻松就写下这种空间，但我们必需考虑实践的可行性。 有人打趣说：「计算机学家就是赶时间的数学家」）

不能简单地用 Transformer 输出这个 概率分布。 目前习惯是，Transformer 输出的 token，会乘上一个矩阵，让输出转换成一支很长的向量，它代表 词典里每个词的概率分布。 但这个 trick 转到 思维空间上就不管用了，因为将所有可能的思维枚举出来 不切实际。

于是我们想到一个办法就是，用「隐式」的方法表示这个概率分布。 关键是一篇名为 “\textit{Hopfield Network is All You Need}” 的论文。 Hochreiter et al 论证，Transformer 是 Hopfield 网络的一个特例。

Hopfield 网络的能量函数可以转变为 概率，那就是 Boltzmann machine.  这个东西在深度学习里占有举足轻重的地位； Hinton、Bengio、LeCun 等人 都深入地研究过它，特别是因为它适用于 强化学习。 它也叫作 EBM (energy-based models).

Transformer 可以轻易地转变为 Hopfield 网络，然后变成 EBM.  那么，根据强化学习的 Bellman 方程，必然可以找到它的 learning update rule, 于是可以建立 AGI 的最基本模型。

这里还有一点细节需要想明白： 在 RL 的「状态」里面有很多逻辑命题，它们互相激活，导致下一个结论命题的出现。 这情况跟大脑里的 ``areas'' 互相激活 很相似。 或许大脑 可以给我们一些启发，如何更有效率地组织 大量 神经元之间的连接？
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{LLM-vs-RL-vs-brain.png}}}
\end{equation}
 
\subsection{RL update rule}

强化学习 的 \textbf{Bellman update} 是根据 某个 output 的 reward, 例如我比较熟悉的 Q-Learning 的 temporal difference update:
\begin{equation}
Q(s,a) \mathrel{+}= \eta\left[ R + \gamma \Delta Q \right]
\end{equation}
$\eta$ = learning rate,\\
$\gamma$ = discount factor,\\
$s$ = state,\\
$a$ = action,\\
而 这个 $Q$ 值可以看成是某种 \textbf{能量}。

下图中，我们比较 LLM, RL 和 大脑。 它们的\textbf{状态} 有什么对应关系？ 
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{LLM-vs-RL-vs-brain.png}}}
\end{equation}

从 model-based RL 的角度看（例如 PILCO），需要 预测世界。 但这不包括预测自己的思想。 但基于感觉资料的思想 包括对世界的预测。 模型就是对世界的思想。 究竟 model-based 跟 model-free 有什么分别？  对 世界的预测 帮助 寻找最佳的对策。 但我说 对世界的预测 是 sensory-based inference 而已。  

在状态之中 某些命题 update，但其他命题 可以不变。 

RL 里面，weights 决定 next state，utility 决定 next state，weights 决定 utility = energy = probability distribution over next states.  但 current state 的 utility value 似乎很难获得？  

State 跟 action 的分别，似乎就是 新命题 跟 命题集合 的分别。 

在大脑中，命题似乎有 位置的固定性。 

\underconst \quad 这部分暂时仍未想清楚....

\section{大脑}


\end{minipage}
\end{preview}

\end{document}
