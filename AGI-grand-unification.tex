\input{../YKY-preamble.tex}
\setmainfont[BoldFont=Alibaba_Sans_Regular.otf,ItalicFont=Alibaba_Sans_Light_Italic.otf]{Alibaba_Sans_Light.otf}
	
\usepackage[active,tightpage]{preview}		% for continuous page(s)
\renewcommand{\PreviewBorder}{0.5cm}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}

\usepackage[absolute,overlay]{textpos}		% for page number on upper left corner

\usepackage{color}
\usepackage{mathtools}
\usepackage[hyperfootnotes=false]{hyperref}

% \usepackage[backend=biber,style=numeric]{biblatex}
% \bibliography{../AGI-book}
% \renewcommand*{\bibfont}{\footnotesize}

\usetikzlibrary{shapes}
\usepackage[export]{adjustbox}				% ??
\usepackage{verbatim} % for comments
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \titleformat{\subsection}[hang]{\bfseries\large\color{blue}}{}{0pt}{} 
% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%

\newcommand\reduline{\bgroup\markoverwith{\textcolor{red}{\rule[-0.5ex]{2pt}{0.4pt}}}\ULon}

%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
% \setmathfont{Latin Modern Math}
\DeclareMathOperator*{\argmin}{arg\,min}

% \usepackage[most]{tcolorbox}
\tcbset{on line, 
	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
	colframe=red,colback=pink,
	highlight math style={enhanced}
}
\newcommand{\atom}{\vcenter{\hbox{\tcbox{....}}}}

\let\oldtextbf\textbf
\renewcommand{\textbf}[1]{\textcolor{blue}{\oldtextbf{#1}}}

\newcommand{\logic}[1]{{\color{violet}{\textit{#1}}}}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\token}{\vcenter{\hbox{\includegraphics[scale=1]{token.png}}}}
\newcommand{\proposition}{\vcenter{\hbox{\includegraphics[scale=0.8]{proposition.png}}}}

\begin{document}

\begin{preview}

\cc{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\LARGE AGI 大统一理论}}
}{
\title{\vspace{-1.5cm} \bfseries\color{blue}{\LARGE AGI Grand Unification}}
}

% \author{YKY} % Your name
\date{\vspace{-2cm}} % Date, can be changed to a custom date

\maketitle

\setcounter{section}{-1}

% (1) Circled page number on upper left corner
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small 1}}}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\section{综述}

\begin{itemize}

	\item 大统一理论是在 \textbf{强化学习} 的框架下进行的，这是以 Richard Sutton 为代表人物 提出的理论框架。 

	\item 在 强化学习里 最辣手的一个问题，就是如何 储存和计算 所有 \textbf{状态} 之上的 \textbf{概率分布}。 对 AGI 来说，状态 = 思维空间。 我们需要的是 所有可能的思维之上的概率分布，而这是 AGI 的一个硬性需求，无法避免。 由于思维空间是高维的向量空间，它上面的概率分布是一个庞大的 mathematical object，很难在计算机上表示。 如果用 神经网络 表示，则问题是如何对这个概率分布进行 \textbf{采样} (sampling), 在神经网络里，这是很困难的。

	\item \textbf{Hopfield 网络}的权重 定义了一个 能量地势 (energy landscape)，它可以看成是一个 implicit 的 \textbf{概率分布}。 透过 Hopfield 网络的 learning，可以改变这个概率分布。 \reduline{但这需要修改 Hopfield 网络的算法，将 能量 诠释成 概率}，而这正是 \textbf{Boltzmann machine}，也称作 EBM (Energy-Based Models).

	\item 根据 ``Hopfield Network is All You Need'' 论文\footnote{感谢 Eric Zeng 给我推荐这篇论文。}，现代 Hopfield 网络的 state update rule 跟 \textbf{Transformer} 重合\footnote{注意这是 state update rule 而不是 learning update rule.  前者 更改 Hopfield 网络的 激活 状态； 后者 更改 Hopfield 网络的权重／记忆。}。 换句话说，每执行一次 Transformer，就会趋向 Hopfield 的能量最低点。 

	\item Transformer 的 softmax 可以看成是 \textbf{大脑}中某种 ``winner-takes-all'' 机制。 从这个角度，可以类比大脑思考的机制，互相参考以获取更多灵感。
	
	\item 我最新的论文 提出，Transformer 具有 \textbf{逻辑结构}，可以在逻辑基础上建立 AGI.

\end{itemize}


\end{minipage}
\end{preview}

\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{AGI 大统一理论}{AGI Unification}}
\end{textblock*}
\vspace*{0.3cm} 

\section{Hopfield 网络}

\subsection{经典 Hopfield 网络}

我们的符号跟随 \textit{Hopfield Network is All You Need}.

$\vect{x}^i$ = 需要记忆的 \textbf{patterns} (有 $N$ 个), $x^i_{s}$ 是它的 $s$-th bit.

$\vect{X} = (\vect{x}^1, ... , \vect{x}^N)$ 是所有 patterns 的矩阵。

$\vect{\xi}$ = 网络的 \textbf{状态}，$\xi_s$ = $s$-th 神经元 的 激活状态。

\textbf{连接权重} between $s$-th and $t$-th neurons:
\begin{equation}
\boxed{\mbox{Weights}} \quad T_{s,t} = \sum_i x^i_s x^i_t
\end{equation}

\textbf{总能量} (Hamiltonian):
\begin{equation}
\boxed{\mbox{Energy}} \quad E = -\frac{1}{2} \sum_s \sum_{t \neq s} \xi_s T_{s,t} \xi_t
\end{equation}

\subsection{Modern Hopfield 网络}

在经典 Hopfield 网络里，当 A 和 B 两个 patterns 太靠近的时候，它们会互相干扰，导致可以储存的 patterns 数量不大。  现代 Hopfield 网络 改变 Hamiltonian 能量函数，令干扰减弱，可以储存数量更多的 patterns：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{modern-Hopfield-network-energy-landscape.png}}}
\end{equation}

\textbf{新的能量}函数:
\begin{equation}
\boxed{\mbox{New Energy}} \quad E = - \sum_i F( \vect{\xi}^T \vect{x}^i )
\end{equation}
注： $ \displaystyle \vect{\xi}^T \vect{x}^i = \sum_s \xi_s x^i_s$ ，\; $F$ = interaction function.

[Demircigil et al 2017] 提出 $F$ 用 exponential 函数。

\textbf{State update rule}:
\begin{equation}
\vect{\xi}^{\mbox{new}} = \vect{X} \; \mbox{softmax}(\beta \vect{X}^T \vect{\xi})
\end{equation}

\subsection{Hopfield-Transformer 对应}

\textbf{传统 Transformer}'s state update rule:
\begin{equation}
\vect{Z} = \mbox{softmax} \left( \frac{1}{\sqrt{d_k}} \vect{Q} \, \vect{K}^T \right) \vect{V}
\end{equation}

\textbf{Modern Hopfield network}'s state update rule:
\begin{equation}
\vect{Z} = \mbox{softmax} \left( \beta \vect{\hat{R}} \vect{\hat{Y}}^T \right) \vect{\hat{Y}}^T
\end{equation}
这里 $\vect{\hat{R}}, \vect{\hat{Y}}, \vect{\hat{X}}$ 是 $\vect{R}, \vect{Y}, \vect{X}$ 分别乘上了适当的 $\vect{W}$'s 矩阵，但为了数式的简洁，使用了代号。

Query patterns = $\vect{R} = (\vect{r}_1, ... , \vect{r}_M)$ 是网络的\textbf{状态}。 之所以有 $M$ 个状态，是因为他们将 Transformer 的 $M$ 个输入 \textbf{摊开}来，才构成一个大的 Hopfield 网。 \reduline{不这样做根本无法将 Hopfield 网和 Transformer 等同起来}。

$\vect{Y}$ 就是 Hopfield 记忆里的 patterns，它们担任 Transformer 里 \textbf{keys} 的角色。

在 Self-Attention 里，$\vect{R} = \vect{Y}$.

\subsection{Boltzmann 机}

注意： 以下是 Boltzmann machine 跟 \textbf{经典} Hopfield network 的对应。

Let $O = (O_1, ..., O_n)$ be the \textbf{state vector}.

$W = \{ W_{s,t} \}$ are connection \textbf{weights}.

\textbf{State update rule}:  $i$-th unit is set to 1 with probability
\begin{equation}
\frac{1}{1 + e^{- S_i / T}}
\end{equation}
where $T$ is a temperature.

如果用以上的 update rule，则 Hopfield 能量 变成 \textbf{概率分布}：
\begin{equation}
P(O) = P(O|W) = \frac{e^{-\mathcal{E}(O)/T}}{Z} \quad \boxed{\mbox{Boltzmann distribution}}
\end{equation}
where partition function $\displaystyle Z = \sum_U e^{-\mathcal{E}(U)/T} $

{\color{red}TO-DO:} \reduline{求出 现代版的 Hopfield network 的 Boltzmann state update rule}.

\subsection{与强化学习 结合}

强化学习 的 \textbf{Bellman update} 是根据 某个 output 的 reward, 例如我比较熟悉的 Q-Learning 的 temporal difference update:
\begin{equation}
Q(s,a) \mathrel{+}= \eta\left[ R + \gamma \Delta Q \right]
\end{equation}
$\eta$ = learning rate,\\
$\gamma$ = discount factor,\\
$s$ = state,\\
$a$ = action,\\
而 这个 $Q$ 值可以看成是某种 \textbf{能量}。

下图中，我们比较 LLM, RL 和 大脑。 它们的\textbf{状态} 有什么对应关系？ 
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{LLM-vs-RL-vs-brain.png}}}
\end{equation}

在状态之中 某些命题 update，但其他命题 可以不变。 

RL 里面，weights 决定 next state，utility 决定 next state，weights 决定 utility = energy = probability distribution over next states.  但 current state 的 utility value 似乎很难获得？  

State 跟 action 的分别，似乎就是 新命题 跟 命题集合 的分别。 

在大脑中，命题似乎有 位置的固定性。 

\underconst \quad 这部分暂时仍未想清楚....

{\color{red}TO-DO:} \reduline{求出 Hopfield-Boltzmann machine 的 \textbf{learning} update rule}.  但它似乎是根据 \textbf{记忆} 而 update 的？？

\end{minipage}
\end{preview}

\begin{comment}
\begin{preview}
\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\begin{textblock*}{20cm}(2.1cm,2cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small 2}}}
	\hspace{8cm}
	\color{blue}{\footnotesize \cc{AGI 大统一理论}{AGI Unification}}
\end{textblock*}
\vspace*{0.3cm} 

\section{大脑}


\end{minipage}
\end{preview}
\end{comment}

\end{document}
