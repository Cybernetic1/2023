回答 子鱼 提出的质疑
=================

首先，一个 token 不一定要储存整个句子的意思，它只需要储存部分的意思，例如数个（但不是很多个）命题的叠加。

在 RL 里，策略函数 需要储存和采样 所有命题之上的概率分布。 

Transformer 每次输出一个 token，它的结构必需这样。

那么这个 token 是否一定要 interpret 为一个概率分布？

但就算 token 是概率分布，但格式是未知的，其实也可以让 token 留在 RL 里，继续训练。这也可以达到 LLM 的效果？（但这仍然有 axis 的问题： 用 RL 做 LLM 的话，会不会仍然保留 axis 讯息？ 所谓 axis 就是 输入词语。 因为 RL 输出「新 token」，它被放回去 state 里面，那就破坏了 axis。 但可能也有保存 axis 的方法？ 就是「回馈」的 tokens 不具备 axis，只有 新词语 的 tokens 有 axis？）

更进一步，我们当然想 诠释 tokens 的内容。 它可能是某些分量之上的概率分布？  但是抽象的描述可能涉及的不确定性较低.... 所以不需要那么高精度的概率？  但很难确定那「将输出 token 乘以矩阵的 trick」是唯一的诠释概率的方法。

再而 解释一下 axis 的问题： 我提出 axis 是 新词语 推导出来的 新命题 的叠加。 那么 RL 不再用 axis 的概念 有没有问题？ 新 token 继续产生新 tokens，直到 它是输出 token？  其实 axis 没有被破坏，而是被复制了、multiplex 了。 换句话说，在新词的 context 以下，它不断循环做推导。 




